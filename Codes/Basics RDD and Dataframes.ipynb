{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiating PySpark / sparksession\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlcontext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Questions \n",
    "***********************************\n",
    "\n",
    "\n",
    "## Question # 1    \n",
    "\n",
    "### Narrow vs Wide Transformation\n",
    "####    1. What are narrow and wide transformations?\n",
    "####    2. Examples of Wide Transformations. Operations of Wide Transformations\n",
    "####    3. Apply those operations on an RDD\n",
    "    \n",
    "### Ans:\n",
    "    Transformations transform an existing RDD to a new RDD. Lazy evaluation since it is only executed and \n",
    "    evaluated only when an action is performed on it.\n",
    "    1.Narrow transformations are 1:1 transformations. The child node is only dependent on the parent node.\n",
    "    Examples of narrow transformations are map(),flatMap(),filter(),union(),sample(),MapPartition(),read(). No data transfer       is     required. Can be computed locally\n",
    "    2.Wide transformations shuffles elements across various partitions i.e. 1:N.Requires data transfer between nodes.\n",
    "    Examples of Wide Transformations: sort(),distinct(),join(),intersection(),OrderBy(),reduceBy(),groupBy()\n",
    "    cartesian(),repartition(),coalesce()\n",
    "        \n",
    "\n",
    "## Question # 2\n",
    "\n",
    "### Difference between Map and FlatMap \n",
    "#### Map transformation takes one element from an RDD and produces one element of a new RDD \n",
    "#### FlatMap transformation takes one element and produces one, zero or more elements of a new RDD\n",
    "\n",
    "\n",
    "## Question # 3\n",
    "\n",
    "### Advantages of spark Dataframes over RDDs?\n",
    "#### Operations using Dataframes are automatically optimized\n",
    "\n",
    "\n",
    "## Question # 4\n",
    "\n",
    "### What is the module called for structured Data processing?\n",
    "##### SparkSQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "************\n",
    "\n",
    "## Question # 1\n",
    "\n",
    "### How to read a text file in PySpark? Show both ways using sparkContext into RDD and reading a file into Dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sc.textFile(\"df.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.text(\"df.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=sqlcontext.read.text(\"df.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question # 2\n",
    "\n",
    "### Splittig an RDD containing string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Student\\t% achieved in the assignments\\tPass exam',\n",
       " 'Bob\\t36%\\tNo',\n",
       " 'Carol\\t95%\\tYes',\n",
       " 'Dan\\t63%\\tYes',\n",
       " 'Eve\\t43%\\tNo',\n",
       " 'Frank\\t84%\\tYes',\n",
       " 'Grace\\t54%\\tYes',\n",
       " 'Heidi\\t15%\\tNo',\n",
       " 'Ivan\\t21%\\tNo',\n",
       " 'Judy\\t91%\\tYes',\n",
       " 'Mallory\\t34%\\tNo']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice\\t70%\\tYes',\n",
       " 'Bob\\t36%\\tNo',\n",
       " 'Carol\\t95%\\tYes',\n",
       " 'Dan\\t63%\\tYes',\n",
       " 'Eve\\t43%\\tNo',\n",
       " 'Frank\\t84%\\tYes',\n",
       " 'Grace\\t54%\\tYes',\n",
       " 'Heidi\\t15%\\tNo',\n",
       " 'Ivan\\t21%\\tNo',\n",
       " 'Judy\\t91%\\tYes',\n",
       " 'Mallory\\t34%\\tNo']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing the first row/header of the RDD\n",
    "firstRow=df.first()\n",
    "\n",
    "df=df.filter(lambda row:row!=firstRow)\n",
    "\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_map=df.map(lambda x:x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alice', '70%', 'Yes'],\n",
       " ['Bob', '36%', 'No'],\n",
       " ['Carol', '95%', 'Yes'],\n",
       " ['Dan', '63%', 'Yes'],\n",
       " ['Eve', '43%', 'No'],\n",
       " ['Frank', '84%', 'Yes'],\n",
       " ['Grace', '54%', 'Yes'],\n",
       " ['Heidi', '15%', 'No'],\n",
       " ['Ivan', '21%', 'No'],\n",
       " ['Judy', '91%', 'Yes'],\n",
       " ['Mallory', '34%', 'No']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMap=words_map.map(lambda wm:Row(student=wm[0],percentage=wm[1],passed=wm[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(student='Alice', percentage='70%', passed='Yes'),\n",
       " Row(student='Bob', percentage='36%', passed='No'),\n",
       " Row(student='Carol', percentage='95%', passed='Yes'),\n",
       " Row(student='Dan', percentage='63%', passed='Yes'),\n",
       " Row(student='Eve', percentage='43%', passed='No'),\n",
       " Row(student='Frank', percentage='84%', passed='Yes'),\n",
       " Row(student='Grace', percentage='54%', passed='Yes'),\n",
       " Row(student='Heidi', percentage='15%', passed='No'),\n",
       " Row(student='Ivan', percentage='21%', passed='No'),\n",
       " Row(student='Judy', percentage='91%', passed='Yes'),\n",
       " Row(student='Mallory', percentage='34%', passed='No')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataMap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMap_df=spark.createDataFrame(dataMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|student|percentage|passed|\n",
      "+-------+----------+------+\n",
      "|  Alice|       70%|   Yes|\n",
      "|    Bob|       36%|    No|\n",
      "|  Carol|       95%|   Yes|\n",
      "|    Dan|       63%|   Yes|\n",
      "|    Eve|       43%|    No|\n",
      "|  Frank|       84%|   Yes|\n",
      "|  Grace|       54%|   Yes|\n",
      "|  Heidi|       15%|    No|\n",
      "|   Ivan|       21%|    No|\n",
      "|   Judy|       91%|   Yes|\n",
      "|Mallory|       34%|    No|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataMap_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question # 3\n",
    "\n",
    "### Working with parallelize function. Creating RDDs from parallelize() and mapping RDD to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating RDD from parallelize\n",
    "courses_RDD=sc.parallelize([\"Big Data\",\"Machine Learning\",\"Intro to Web Science\"\\\n",
    "                            ,\"Network Theory\",\"NPDEs\",\"Data Science\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[19] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'Machine Learning',\n",
       " 'Intro to Web Science',\n",
       " 'Network Theory',\n",
       " 'NPDEs',\n",
       " 'Data Science']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting/mapping data into various rows\n",
    "courses_map=courses_RDD.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Big Data'],\n",
       " ['Machine Learning'],\n",
       " ['Intro to Web Science'],\n",
       " ['Network Theory'],\n",
       " ['NPDEs'],\n",
       " ['Data Science']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving a name to each row\n",
    "courses=courses_map.map(lambda x:Row(course=x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(course='Big Data'),\n",
       " Row(course='Machine Learning'),\n",
       " Row(course='Intro to Web Science'),\n",
       " Row(course='Network Theory'),\n",
       " Row(course='NPDEs'),\n",
       " Row(course='Data Science')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe from the updated RDD\n",
    "courses_df=spark.createDataFrame(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[course: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              course|\n",
      "+--------------------+\n",
      "|            Big Data|\n",
      "|    Machine Learning|\n",
      "|Intro to Web Science|\n",
      "|      Network Theory|\n",
      "|               NPDEs|\n",
      "|        Data Science|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming the created RDD to a new RDD with key value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'Machine Learning',\n",
       " 'Intro to Web Science',\n",
       " 'Network Theory',\n",
       " 'NPDEs',\n",
       " 'Data Science']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial RDD created from parallelize function\n",
    "courses_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the old rdd to a new RDD with key value pairs\n",
    "\n",
    "key_val_rdd=courses_RDD.map(lambda x:(x,\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big Data', 'a'),\n",
       " ('Machine Learning', 'a'),\n",
       " ('Intro to Web Science', 'a'),\n",
       " ('Network Theory', 'a'),\n",
       " ('NPDEs', 'a'),\n",
       " ('Data Science', 'a')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_val_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question # 4\n",
    "\n",
    "### Applying filter to an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data',\n",
       " 'Machine Learning',\n",
       " 'Intro to Web Science',\n",
       " 'Network Theory',\n",
       " 'NPDEs',\n",
       " 'Data Science']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_courses=courses_RDD.filter(lambda x: x.startswith(\"B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_courses.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Big Data', 'Data Science']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example\n",
    "filtered_courses2=courses_RDD.filter(lambda x: \"Data\" in x)\n",
    "filtered_courses2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Frames \n",
    "_____________________________________\n",
    "\n",
    "## Question # 1\n",
    "\n",
    "\n",
    "### Read the flightdata csv file (link mentioned below) which has three columns Destination Country Name, Origin Country Name and Count. Your task is to learn about the structure (data types) of data,\n",
    "\n",
    "https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/flight-data/csv/2015-summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data into a dataframe\n",
    "flight_df=spark.read.csv(\"2015-summary.csv\",header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'int')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question # 2\n",
    "\n",
    "### Find the maximum number of flights from Origin country to Destination country by reading flightdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|     United States|      United States|370002|\n",
      "|     United States|             Canada|  8483|\n",
      "|            Canada|      United States|  8399|\n",
      "|     United States|             Mexico|  7187|\n",
      "|            Mexico|      United States|  7140|\n",
      "|    United Kingdom|      United States|  2025|\n",
      "|     United States|     United Kingdom|  1970|\n",
      "|             Japan|      United States|  1548|\n",
      "|     United States|              Japan|  1496|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|     United States|            Germany|  1336|\n",
      "|       South Korea|      United States|  1048|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|     United States|             France|   952|\n",
      "|            France|      United States|   935|\n",
      "|     United States|              China|   920|\n",
      "|          Colombia|      United States|   873|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.sort(flight_df[\"count\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.agg({\"count\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question # 3\n",
    "\n",
    "### Count those rows where number of flights going from Origin country to Destination country are greater than 10 by reading flightdata again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.filter(flight_df[\"count\"]>10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question # 4\n",
    "\n",
    "### Count total number of flights having destination country name is United States.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411352|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.filter(flight_df[\"DEST_COUNTRY_NAME\"]==\"United States\").agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   DEST_COUNTRY_NAME|sum(count)|\n",
      "+--------------------+----------+\n",
      "|              Zambia|         1|\n",
      "|           Venezuela|       290|\n",
      "|             Uruguay|        43|\n",
      "|       United States|    411352|\n",
      "|      United Kingdom|      2025|\n",
      "|United Arab Emirates|       320|\n",
      "|             Ukraine|        14|\n",
      "|Turks and Caicos ...|       230|\n",
      "|              Turkey|       138|\n",
      "|             Tunisia|         3|\n",
      "| Trinidad and Tobago|       211|\n",
      "|         The Bahamas|       955|\n",
      "|            Thailand|         3|\n",
      "|              Taiwan|       266|\n",
      "|         Switzerland|       294|\n",
      "|              Sweden|       118|\n",
      "|            Suriname|         1|\n",
      "|               Spain|       420|\n",
      "|         South Korea|      1048|\n",
      "|        South Africa|        36|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.groupBy(\"DEST_COUNTRY_NAME\").agg({\"count\":\"sum\"}).sort(\"DEST_COUNTRY_NAME\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
